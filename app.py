from uuid import uuid4

import gradio as gr

from src.assistant import Assistant


assistant = Assistant(service_cfg_path="configs/deployment_config.yaml")


def _build_delete_request(repo_url: str) -> dict:
    return {
        "meta": {"request_id": str(uuid4())},
        "repo_url": repo_url,
    }


def _build_index_request(repo_url: str) -> tuple[dict, dict]:
    request = {
        "meta": {"request_id": str(uuid4())},
        "repo_url": repo_url,
        "branch": "main",
    }

    config = {
        "ast_chunker_config": {
            "max_chunk_size": 1000,
            "chunk_overlap": 50,
            "extensions": [
                ".py",
                ".ipynb",
                ".cpp",
                ".h",
                ".java",
                ".ts",
                ".tsx",
                ".cs",
            ],
            "chunk_expansion": True,
            "metadata_template": "default",
        },
        "text_splitter_config": {"chunk_size": 500, "chunk_overlap": 50},
        "exclude_patterns": ["*.lock", "__pycache__", ".venv", "build"],
    }

    return request, config


def _build_search_config() -> dict:
    return {
        "query_preprocessor": {
            "enabled": True,
            "normalize_whitespace": True,
            "sanitization": {
                "enabled": True,
                "regex_patterns": ["jailbreak", "hallucinations"],
                "replacement_token": ""
            }
        },
        "query_rewriter": {"enabled": False},
        "retriever": {"enabled": True},
        "filtering": {"enabled": True},
        "reranker": {"enabled": False},
        "context_expansion": {"enabled": True},
        "qa": {"enabled": True},
        "query_postprocessor": {
            "enabled": True,
            "format_markdown": True,
            "sanitization": {
                "enabled": True,
                "regex_patterns": ["can't", "wtf"],
                "replacement_token": ""
            }
        }
    }


def _build_agent_config(
    max_iterations: int = 5,
    max_time_seconds: float = 120.0,
    confidence_threshold: float = 0.7,
    min_relevant_chunks: int = 3,
    relevance_score_threshold: float = 0.5,
    enable_query_refinement: bool = True,
    enable_filter_adjustment: bool = True,
    enable_retriever_adjustment: bool = True,
    generate_final_answer: bool = True,
    use_llm: bool = True,
    llm_model: str = "openai/gpt-oss-120b"
) -> dict:
    """ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ deep research."""
    config = {
        "max_iterations": max_iterations,
        "max_time_seconds": max_time_seconds,
        "confidence_threshold": confidence_threshold,
        "min_relevant_chunks": min_relevant_chunks,
        "relevance_score_threshold": relevance_score_threshold,
        "enable_query_refinement": enable_query_refinement,
        "enable_filter_adjustment": enable_filter_adjustment,
        "enable_retriever_adjustment": enable_retriever_adjustment,
        "generate_final_answer": generate_final_answer
    }

    if use_llm:
        config["llm_config"] = {
            "provider": "openrouter",
            "model_name": llm_model,
            "parameters": {
                "temperature": 0.1,
                "max_tokens": 4096
            }
        }

    # Initial Search Engine configuration
    config["initial_search_config"] = {
        "retriever": {
            "size": 10,
            "threshold": 0.3,
            "bm25_weight": 0.3
        },
        "reranker": {
            "enabled": True,
            "top_k": 5,
            "threshold": 0.4
        },
        "qa": {"enabled": False}
    }

    return config


async def index_repo(repo_url: str) -> str:
    if not repo_url:
        return "âŒ **ĞÑˆĞ¸Ğ±ĞºĞ°:** Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ GitHub URL."

    request, config = _build_index_request(repo_url)

    try:
        response = await assistant.index(request, config)

        # Calculate duration
        duration = (
            response.meta.end_datetime - response.meta.start_datetime
        ).total_seconds()

        # Build verbose response
        result = []
        result.append("## ğŸ“Š Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸\n")
        result.append(f"**Request ID:** `{response.meta.request_id}`\n")
        result.append(f"**Repository URL:** {response.repo_url}\n")
        result.append(f"**Ğ’Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ:** {duration:.2f} ÑĞµĞºÑƒĞ½Ğ´\n")
        result.append(f"**Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ:** {response.meta.status}\n")

        # Check if repo was already indexed
        is_already_indexed = (
            response.job_status.description_error
            and "already indexed" in response.job_status.description_error.lower()
        )

        if is_already_indexed:
            result.append("\nâš ï¸ **Ğ ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½**\n")
            result.append(
                "Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ°, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ "
                "ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….\n"
            )
        else:
            # Show job status details
            if response.job_status.status:
                status_emoji = {
                    "failed": "âŒ",
                    "loaded": "ğŸ“¥",
                    "parsed": "ğŸ”",
                    "vectorized": "ğŸ§®",
                    "saved_to_qdrant": "âœ…",
                }
                emoji = status_emoji.get(response.job_status.status, "â„¹ï¸")
                result.append(
                    f"\n**Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸:** {emoji} {response.job_status.status}\n"
                )

            # Show chunks processed
            if response.job_status.chunks_processed is not None:
                result.append(
                    f"**ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²:** {response.job_status.chunks_processed}\n"
                )

            # Show errors if any
            if response.meta.status == "error":
                result.append("\n### âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸\n")
                if response.job_status.description_error:
                    result.append(
                        f"**ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸:**\n```\n"
                        f"{response.job_status.description_error}\n```\n"
                    )
                else:
                    result.append("ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸.\n")
            elif response.job_status.status == "saved_to_qdrant":
                result.append("\n### âœ… Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾\n")
                result.append(
                    "Ğ ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½ "
                    "Ğ² Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….\n"
                )

        return "".join(result)

    except Exception as e:
        return f"âŒ **ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°:** {type(e).__name__}: {str(e)}"


async def delete_index(repo_url: str) -> str:
    if not repo_url:
        return "âŒ **ĞÑˆĞ¸Ğ±ĞºĞ°:** Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ GitHub URL."

    request = _build_delete_request(repo_url)

    try:
        response = await assistant.delete_index(request)

        # Calculate duration
        duration = (
            response.meta.end_datetime - response.meta.start_datetime
        ).total_seconds()

        # Build verbose response
        result = []
        result.append("## ğŸ—‘ï¸ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°\n")
        result.append(f"**Request ID:** `{response.meta.request_id}`\n")
        result.append(f"**Repository URL:** {response.repo_url}\n")
        result.append(f"**Ğ’Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ:** {duration:.2f} ÑĞµĞºÑƒĞ½Ğ´\n")
        result.append(f"**Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ:** {response.meta.status}\n")

        if response.success:
            result.append("\n### âœ… Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾\n")
            result.append(
                "Ğ˜Ğ½Ğ´ĞµĞºÑ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½ Ğ¸Ğ· Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….\n"
            )
            if response.message:
                result.append(f"\n**Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ:** {response.message}\n")
        else:
            result.append("\n### âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğ¸\n")
            if response.message:
                result.append(f"**ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸:**\n```\n{response.message}\n```\n")
            else:
                result.append("ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°.\n")

        return "".join(result)

    except Exception as e:
        return f"âŒ **ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°:** {type(e).__name__}: {str(e)}"


def _collect_sources(response) -> list[dict]:
    sources = []
    if getattr(response, "sources", None):
        for source in response.sources:
            sources.append(
                {
                    "filepath": source.metadata.filepath,
                    "language": source.metadata.language or "",
                    "content": source.content,
                }
            )
    return sources


def _render_sources(sources: list[dict], show_sources: bool) -> str:
    if not sources:
        return "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:\n- Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾\n"

    sources_md = "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:\n"
    for source in sources:
        sources_md += f"- {source['filepath']}\n"
        if show_sources:
            sources_md += f"\n```{source['language']}\n"
            sources_md += f"{source['content']}\n"
            sources_md += "```\n"
    return sources_md


def _render_sources_detailed(sources: list[dict], show_content: bool) -> str:
    """Ğ ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°."""
    if not sources:
        return "### ğŸ“š Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸\n\n*Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹*\n"

    sources_md = "### ğŸ“š ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸\n\n"
    for i, source in enumerate(sources, 1):
        score = source.get("reranker_score") or source.get("retrieval_score") or 0
        sources_md += f"**{i}. `{source['filepath']}`**\n"
        sources_md += f"   - Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ¸: {source.get('start_line', '?')}-{source.get('end_line', '?')}\n"
        sources_md += f"   - Ğ¯Ğ·Ñ‹Ğº: {source.get('language') or 'Ğ½Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½'}\n"
        sources_md += f"   - Ğ ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ: {score:.3f}\n"

        if show_content:
            sources_md += f"\n```{source.get('language', '')}\n"
            sources_md += f"{source['content']}\n"
            sources_md += "```\n"
        sources_md += "\n"

    return sources_md


def _content_to_text(content) -> str:
    if content is None:
        return ""
    if isinstance(content, str):
        return content
    if isinstance(content, dict):
        return str(content.get("text") or content.get("content") or "")
    if isinstance(content, list):
        parts = []
        for item in content:
            if isinstance(item, str):
                parts.append(item)
            elif isinstance(item, dict):
                if item.get("type") == "text":
                    parts.append(str(item.get("text", "")))
                elif "text" in item:
                    parts.append(str(item.get("text", "")))
        return "".join(parts)
    return str(content)


def _normalize_history(history: list[dict] | None) -> list[dict]:
    """Ensure history is list of {'role': str, 'content': str}."""
    if not history:
        return []
    out = []
    for m in history:
        if isinstance(m, dict) and "role" in m:
            out.append(
                {
                    "role": str(m.get("role", "")),
                    "content": _content_to_text(m.get("content")),
                }
            )
    return out


def _last_pairs(history: list[dict], pairs: int = 3) -> list[dict]:
    """Keep last N user+assistant pairs = 2*N messages."""
    max_msgs = 2 * pairs
    return history if len(history) <= max_msgs else history[-max_msgs:]


async def chat(
    repo_url: str,
    message: str,
    show_sources: bool,
    history_state: list[dict],
    chatbot_history: list[dict],
):
    history_state = _normalize_history(history_state)
    chatbot_history = _normalize_history(chatbot_history)

    if not repo_url:
        return (
            "Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ URL Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ.",
            "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:\n- Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾\n",
            [],
            history_state,
            chatbot_history,
        )

    if not message:
        return (
            "Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ.",
            "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:\n- Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾\n",
            [],
            history_state,
            chatbot_history,
        )

    # Backend context: last 3 Q/A pairs (6 msgs) + new question
    context_messages = _last_pairs(history_state, pairs=3)
    request_messages = context_messages + [{"role": "user", "content": message}]

    request = {
        "meta": {"request_id": str(uuid4())},
        "query": {"messages": request_messages},
        "repo_url": repo_url,
    }
    config = _build_search_config()

    try:
        response = await assistant.query(request, config)
        answer_text = (getattr(response, "answer", "") or "").strip()
    except Exception as e:
        return (
            f"ĞÑˆĞ¸Ğ±ĞºĞ°: {type(e).__name__}: {e}",
            "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:\n- Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾\n",
            [],
            history_state,
            chatbot_history,
        )

    final_answer = answer_text or "ĞÑ‚Ğ²ĞµÑ‚ Ğ¿ÑƒÑÑ‚."

    sources = _collect_sources(response)
    sources_md = _render_sources(sources, show_sources)

    chatbot_history = chatbot_history + [
        {"role": "user", "content": message},
        {"role": "assistant", "content": final_answer},
    ]

    new_history_state = request_messages + [
        {"role": "assistant", "content": final_answer}
    ]
    new_history_state = _last_pairs(new_history_state, pairs=3)

    return sources_md, sources, new_history_state, chatbot_history


def update_sources(show_sources: bool, sources: list[dict]):
    return _render_sources(sources or [], show_sources)


async def agent_research(
    repo_url: str,
    question: str,
    max_iterations: int,
    max_time_seconds: float,
    confidence_threshold: float,
    min_relevant_chunks: int,
    relevance_score_threshold: float,
    enable_query_refinement: bool,
    enable_filter_adjustment: bool,
    enable_retriever_adjustment: bool,
    generate_final_answer: bool,
    use_llm: bool,
    llm_model: str,
    show_sources_content: bool
):
    """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ."""

    if not repo_url:
        return (
            "âŒ **ĞÑˆĞ¸Ğ±ĞºĞ°:** Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ GitHub URL Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ.",
            "",
            []
        )

    if not question:
        return (
            "âŒ **ĞÑˆĞ¸Ğ±ĞºĞ°:** Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.",
            "",
            []
        )

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
    request = {
        "meta": {"request_id": str(uuid4())},
        "query": {"messages": [{"role": "user", "content": question}]},
        "repo_url": repo_url,
    }

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°
    config = _build_agent_config(
        max_iterations=max_iterations,
        max_time_seconds=max_time_seconds,
        confidence_threshold=confidence_threshold,
        min_relevant_chunks=min_relevant_chunks,
        relevance_score_threshold=relevance_score_threshold,
        enable_query_refinement=enable_query_refinement,
        enable_filter_adjustment=enable_filter_adjustment,
        enable_retriever_adjustment=enable_retriever_adjustment,
        generate_final_answer=generate_final_answer,
        use_llm=use_llm,
        llm_model=llm_model
    )

    try:
        response = await assistant.deep_research(request, config)

        duration = (
            response.meta.end_datetime - response.meta.start_datetime
        ).total_seconds()

        result_parts = []
        result_parts.append("## ğŸ”¬ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ\n\n")
        result_parts.append(f"**Request ID:** `{response.meta.request_id}`\n\n")
        result_parts.append(f"**Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ:** {response.meta.status}\n\n")
        result_parts.append(f"**Ğ’Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ:** {duration:.2f} ÑĞµĞº.\n\n")
        result_parts.append(f"**Ğ ĞµĞ¶Ğ¸Ğ¼:** {response.status}\n\n")
        result_parts.append("---\n\n")
        result_parts.append("### ğŸ’¡ ĞÑ‚Ğ²ĞµÑ‚\n\n")
        result_parts.append(response.answer or "*ĞÑ‚Ğ²ĞµÑ‚ Ğ½Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½*")
        result_parts.append("\n")

        result_md = "".join(result_parts)

        sources = _collect_sources(response)
        sources_md = _render_sources_detailed(sources, show_sources_content)

        return result_md, sources_md, sources

    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        return (
            f"âŒ **ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°:** {type(e).__name__}: {str(e)}\n\n"
            f"```\n{error_trace}\n```",
            "",
            []
        )


def update_agent_sources(show_content: bool, sources: list[dict]):
    """ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°."""
    return _render_sources_detailed(sources or [], show_content)


with gr.Blocks(title="RAGCode") as demo:
    gr.Markdown("# RAGCode")

    with gr.Tabs():
        with gr.Tab("ğŸ—‚ï¸ Ğ˜Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹"):
            repo_url_input = gr.Textbox(label="GitHub URL")
            with gr.Row():
                index_button = gr.Button("Ğ˜Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ", variant="primary")
                delete_button = gr.Button("Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ğ´ĞµĞºÑ", variant="stop")
            index_status = gr.Markdown()
            index_button.click(index_repo, inputs=repo_url_input, outputs=index_status)
            delete_button.click(
                delete_index, inputs=repo_url_input, outputs=index_status
            )

        with gr.Tab("ğŸ’¬ Ğ§Ğ°Ñ‚ Ğ¿Ğ¾ ĞºĞ¾Ğ´Ñƒ"):
            chat_repo_url = gr.Textbox(label="URL Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ")

            chatbot = gr.Chatbot(label="Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ", height=420)

            sources = gr.Markdown("Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:\n- Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾\n")
            message_input = gr.Textbox(label="Ğ’Ğ°Ñˆ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ")
            show_sources = gr.Checkbox(
                label="ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²", value=False
            )

            sources_state = gr.State([])
            history_state = gr.State([])

            send_button = gr.Button("Ğ¡Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ")

            send_button.click(
                chat,
                inputs=[
                    chat_repo_url,
                    message_input,
                    show_sources,
                    history_state,
                    chatbot,
                ],
                outputs=[sources, sources_state, history_state, chatbot],
            )

            show_sources.change(
                update_sources,
                inputs=[show_sources, sources_state],
                outputs=[sources],
            )

        with gr.Tab("ğŸ”¬ ĞĞ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº"):
            gr.Markdown("""
            ## Ğ£Ğ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
            
            ĞĞ³ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ 
            Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² 
            Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².
            """)

            with gr.Row():
                with gr.Column(scale=2):
                    agent_repo_url = gr.Textbox(
                        label="GitHub URL Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ",
                        placeholder="https://github.com/owner/repo",
                    )
                    agent_question = gr.Textbox(
                        label="Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                        placeholder="ĞšĞ°Ğº Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğµ?",
                        lines=3,
                    )

                with gr.Column(scale=1):
                    gr.Markdown("### âš™ï¸ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°")

                    with gr.Accordion("ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸", open=True):
                        agent_max_iterations = gr.Slider(
                            minimum=1,
                            maximum=20,
                            value=5,
                            step=1,
                            label="ĞœĞ°ĞºÑ. Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹",
                            info="ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                        )
                        agent_max_time = gr.Slider(
                            minimum=10,
                            maximum=300,
                            value=120,
                            step=10,
                            label="ĞœĞ°ĞºÑ. Ğ²Ñ€ĞµĞ¼Ñ (ÑĞµĞº)",
                            info="Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°",
                        )
                        agent_confidence = gr.Slider(
                            minimum=0.0,
                            maximum=1.0,
                            value=0.7,
                            step=0.05,
                            label="ĞŸĞ¾Ñ€Ğ¾Ğ³ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸",
                            info="ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸",
                        )

                    with gr.Accordion("ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸", open=False):
                        agent_min_chunks = gr.Slider(
                            minimum=1,
                            maximum=20,
                            value=3,
                            step=1,
                            label="ĞœĞ¸Ğ½. Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²",
                        )
                        agent_relevance_threshold = gr.Slider(
                            minimum=0.0,
                            maximum=1.0,
                            value=0.5,
                            step=0.05,
                            label="ĞŸĞ¾Ñ€Ğ¾Ğ³ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡Ğ°Ğ½ĞºĞ°",
                        )

                    with gr.Accordion("Ğ Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°", open=False):
                        agent_enable_query_refinement = gr.Checkbox(
                            label="ĞŸĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°",
                            value=True,
                            info="Ğ Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ",
                        )
                        agent_enable_filter_adjustment = gr.Checkbox(
                            label="ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²",
                            value=True,
                            info="Ğ Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹",
                        )
                        agent_enable_retriever_adjustment = gr.Checkbox(
                            label="ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ°",
                            value=True,
                            info="Ğ Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                        )
                        agent_generate_answer = gr.Checkbox(
                            label="Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚",
                            value=True,
                            info="Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ LLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°",
                        )

                    with gr.Accordion("ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ LLM", open=False):
                        agent_use_llm = gr.Checkbox(
                            label="Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ LLM Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°",
                            value=True,
                            info="Ğ•ÑĞ»Ğ¸ Ğ²Ñ‹ĞºĞ»ÑÑ‡ĞµĞ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸",
                        )
                        agent_llm_model = gr.Dropdown(
                            choices=[
                                "openai/gpt-oss-120b",
                                "openrouter/anthropic/claude-3.5-sonnet",
                                "mistral-large-latest",
                                "GigaChat-2-Max",
                            ],
                            value="openai/gpt-oss-120b",
                            label="ĞœĞ¾Ğ´ĞµĞ»ÑŒ LLM",
                        )

            agent_run_button = gr.Button(
                "ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                variant="primary",
                size="lg",
            )

            gr.Markdown("---")

            with gr.Row():
                with gr.Column(scale=2):
                    agent_result = gr.Markdown(
                        value="*Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ²ÑÑ‚ÑÑ Ğ·Ğ´ĞµÑÑŒ...*",
                        label="Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚",
                    )

                with gr.Column(scale=1):
                    agent_show_sources_content = gr.Checkbox(
                        label="ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²",
                        value=False,
                    )
                    agent_sources = gr.Markdown(
                        value="### ğŸ“š Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸\n\n*Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ¾ÑĞ²ÑÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°*",
                        label="Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸",
                    )

            agent_sources_state = gr.State([])

            agent_run_button.click(
                agent_research,
                inputs=[
                    agent_repo_url,
                    agent_question,
                    agent_max_iterations,
                    agent_max_time,
                    agent_confidence,
                    agent_min_chunks,
                    agent_relevance_threshold,
                    agent_enable_query_refinement,
                    agent_enable_filter_adjustment,
                    agent_enable_retriever_adjustment,
                    agent_generate_answer,
                    agent_use_llm,
                    agent_llm_model,
                    agent_show_sources_content,
                ],
                outputs=[
                    agent_result,
                    agent_sources,
                    agent_sources_state,
                ],
            )

            agent_show_sources_content.change(
                update_agent_sources,
                inputs=[agent_show_sources_content, agent_sources_state],
                outputs=[agent_sources],
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=8501)
